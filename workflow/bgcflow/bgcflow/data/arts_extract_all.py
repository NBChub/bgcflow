import ast
import json
import logging
import sys
from pathlib import Path

import numpy as np
import pandas as pd

log_format = "%(levelname)-8s %(asctime)s   %(message)s"
date_format = "%d/%m %H:%M:%S"
logging.basicConfig(format=log_format, datefmt=date_format, level=logging.DEBUG)


def generate_arts_dict(df_arts):
    """
    Generate a dictionary representation of the ARTS BGC table.

    This function converts the provided ARTS BGC table DataFrame to a dictionary,
    with scaffolds as keys. Each key maps to another dictionary containing the
    count of occurrences ("counts") and a list of regions ("regions"), each represented
    as a dictionary with keys "cluster_id", "products", and "location".

    Parameters
    ----------
    df_arts : pandas.DataFrame
        The ARTS table DataFrame to be converted. It should have columns "Source",
        "#Cluster", "Type", and "Location".

    Returns
    -------
    dict
        The generated dictionary representing the ARTS table
    """
    arts_dict = {}
    arts_scaffold_count = df_arts.Source.value_counts().to_dict()
    for k in arts_scaffold_count.keys():
        arts_dict[k] = {"counts": arts_scaffold_count[k]}
        arts_dict[k]["regions"] = []
        for i in df_arts[df_arts.loc[:, "Source"] == k].index:
            regions = {
                "cluster_id": df_arts.loc[i, "#Cluster"],
                "products": df_arts.loc[i, "Type"],
                "location": df_arts.loc[i, "Location"],
            }
            arts_dict[k]["regions"].append(regions)
    return arts_dict


def generate_arts_mapping(df_arts, as_json):
    """
    Generate mappings between ARTS and antiSMASH results.

    This function generates mappings between ARTS table and antiSMASH JSON,
    based on the number of detected regions and their specific details like
    products and location. The function iterates over the records in the
    provided antiSMASH JSON and finds corresponding matches in the ARTS
    table, returning dictionaries mapping cluster IDs to BGC IDs and contig IDs.

    Parameters
    ----------
    df_arts : pandas.DataFrame
        The ARTS table DataFrame to be mapped. It should have columns "Source",
        "#Cluster", "Type", and "Location".

    as_json : dict
        The antiSMASH results in JSON format, converted to a Python dictionary.

    Returns
    -------
    tuple
        Returns a tuple containing three dictionaries:

        hit_mapper : dict
            A dictionary mapping cluster IDs from the ARTS table to BGC IDs
            from antiSMASH results.

        arts_dict : dict
            A dictionary representation of the ARTS table generated by the
            generate_arts_dict function.

        contig_mapper : dict
            A dictionary mapping cluster IDs from the ARTS table to contig IDs
            from antiSMASH results.
    """
    arts_dict = generate_arts_dict(df_arts)
    # container for final result
    hit_mapper = {}
    contig_mapper = {}

    # iterate antismash json records
    for num, r in enumerate(as_json["records"]):
        # count number of detected regions per record
        region_count = len(r["areas"])
        # if antismash detects region
        if region_count > 0:
            contig_id = r["id"]
            logging.info(
                f"Contig {contig_id} has {region_count} regions detected. Finding corresponding scaffold in ARTS2..."
            )
            # find arts scaffold with the same region number detected
            arts_match = [
                k
                for k in arts_dict.keys()
                if int(arts_dict[k]["counts"]) == int(region_count)
            ]
            logging.debug(f"Finding matches from: {arts_match}")
            for n, a in enumerate(r["areas"]):
                bgc_id = f"{contig_id}.region{str(n+1).zfill(3)}"
                location = f"{a['start']} - {a['end']}"
                products = ",".join(a["products"])
                logging.debug(f"Finding match for: {bgc_id} | {location} | {products}")
                for m in arts_match:
                    bgc_match = arts_dict[m]["regions"][n]
                    if (bgc_match["location"] == location) and (
                        bgc_match["products"] == products
                    ):
                        logging.debug(
                            f"Found match! {bgc_match['cluster_id']} | {bgc_match['location']} | {bgc_match['products']}"
                        )
                        # logging.debug(f"Found match! {contig_id} == ARTS2 {m}")
                        hit_mapper[bgc_match["cluster_id"]] = bgc_id
                        contig_mapper[bgc_match["cluster_id"]] = contig_id
    return hit_mapper, arts_dict, contig_mapper


def extract_arts_coretable(
    arts_coretable_tsv, outfile_summary, outfile_hits=None, genome_id=None
):
    """
    Extract information from ARTS core table and write to JSON files.

    This function reads the ARTS core table from a TSV file, generates primary keys
    for the core table, extracts hits, and writes the core table and hits information
    to JSON files. It returns a DataFrame representation of the hits information.

    Parameters
    ----------
    arts_coretable_tsv : str or pathlib.Path
        Path to the ARTS core table TSV file.

    outfile_summary : str or pathlib.Path
        Path to the output JSON file where the core table information will be written.

    outfile_hits : str or pathlib.Path, optional
        Path to the output JSON file where the hits information will be written.
        If not provided, the hits information will not be written to a file.

    genome_id : str, optional
        Identifier for the genome. If not provided, it will be inferred from the
        core table information.

    Returns
    -------
    pandas.DataFrame
        A DataFrame representation of the extracted hits information, with each row
        representing a hit and columns representing hit attributes.

    Example
    -------
    >>> extract_arts_coretable('path/to/coretable.tsv', 'path/to/summary.json', 'path/to/hits.json', 'genome_id')
    >>> # This will read the core table from 'path/to/coretable.tsv', write summary
    >>> # information to 'path/to/summary.json', write hits information to 'path/to/hits.json',
    >>> # and return a DataFrame representing the hits information.
    """
    logging.info(f"Extracting ARTS core table information from: {arts_coretable_tsv}")
    arts_coretable = pd.read_csv(arts_coretable_tsv, sep="\t").fillna("")

    logging.info("Generating primary key for core_table...")
    arts_coretable.index = [
        f"{genome_id}__arts_core__{i}" for i in arts_coretable["#Core_gene"]
    ]
    arts_coretable["genome_id"] = genome_id
    arts_coretable_dict = arts_coretable.T.to_dict()
    logging.info(f"Writing Core Table JSON to: {outfile_summary}")
    with open(outfile_summary, "w") as f:
        json.dump(arts_coretable_dict, f, indent=2)

    logging.info("Extracting hits...")
    arts_coretable_hits = {}
    for i in arts_coretable.index:
        hits = arts_coretable.loc[i, "[Hits_listed]"].split(";")
        core_gene = arts_coretable.loc[i, "#Core_gene"]
        for num, h in enumerate(hits):
            data = h.strip("[").strip("]").split("|")
            scaffold, sequence_id = data[4].split("#")[-1].split("=")[-1].split("_")
            if genome_id is None:
                genome_id = data[0].split(" ")[-1]
            result = {
                "core_gene": core_gene,
                "genome_id": genome_id,
                "name": data[1],
                "product": data[2],
                "start": data[4].split("#")[0].split()[0],
                "stop": data[4].split("#")[0].split()[1],
                "strand": data[4].split("#")[0].split()[2],
                "scaffold": f"scaffold_{scaffold}",
                "sequence_id": sequence_id,
                "type": data[5].split("=")[-1],
                "transl_table": data[6].split("=")[-1],
                "core_table_fkey": f"{genome_id}__arts_core__{core_gene}",
                "duplication": arts_coretable.loc[i, "Duplication"],
                # "bgc_proximity" : arts_coretable.loc[i, "BGC_Proximity"],
                "phylogeny": arts_coretable.loc[i, "Phylogeny"],
                "known_target": arts_coretable.loc[i, "Known_target"],
            }

            arts_coretable_hits[
                f"{genome_id}__{core_gene}__{scaffold}__{sequence_id}"
            ] = result

    if outfile_hits is not None:
        outfile_hits.parent.mkdir(parents=True, exist_ok=True)
        logging.info(f"Writing Core Hits JSON to: {outfile_hits}")
        with open(outfile_hits, "w") as f:
            json.dump(arts_coretable_hits, f, indent=2)

    return pd.DataFrame.from_dict(arts_coretable_hits).T


def extract_arts_bgctable(
    arts_bgctable_tsv,
    outfile_summary,
    outfile_hits=None,
    hit_mapper=None,
    genome_id=None,
):
    """
    Extract information from ARTS BGC table and write to JSON files.

    This function reads the ARTS BGC table from a TSV file, optionally corrects
    the #Cluster name using a provided mapping, extracts hits, and writes the
    BGC table and hits information to JSON files. It returns a DataFrame
    representation of the hits information.

    Parameters
    ----------
    arts_bgctable_tsv : str or pathlib.Path
        Path to the ARTS BGC table TSV file.

    outfile_summary : str or pathlib.Path
        Path to the output JSON file where the BGC table information will be written.

    outfile_hits : str or pathlib.Path, optional
        Path to the output JSON file where the hits information will be written.
        If not provided, the hits information will not be written to a file.

    hit_mapper : dict, optional
        A dictionary for correcting #Cluster names in the BGC table. If provided,
        the #Cluster names in the BGC table will be corrected based on this mapping.

    genome_id : str, optional
        Identifier for the genome. If not provided, it will be inferred from the
        BGC table information.

    Returns
    -------
    pandas.DataFrame
        A DataFrame representation of the extracted hits information, with each row
        representing a hit and columns representing hit attributes.

    Example
    -------
    >>> extract_arts_bgctable('path/to/bgctable.tsv', 'path/to/summary.json', 'path/to/hits.json', hit_mapper={'cluster_1': 'bgc_1'}, genome_id='genome_id')
    >>> # This will read the BGC table from 'path/to/bgctable.tsv', write summary
    >>> # information to 'path/to/summary.json', write hits information to 'path/to/hits.json',
    >>> # and return a DataFrame representing the hits information.
    """
    logging.info(f"Extracting ARTS BGC table information from: {arts_bgctable_tsv}")
    arts_bgctable = pd.read_csv(arts_bgctable_tsv, sep="\t").fillna("")
    if hit_mapper is None:
        arts_bgctable.index = [i for i in arts_bgctable["#Cluster"]]
    elif type(hit_mapper) == dict:
        logging.info("Correcting #Cluster name...")
        arts_bgctable["bgc_id"] = [
            f"{hit_mapper[i]}" for i in arts_bgctable["#Cluster"]
        ]
        arts_bgctable["genome_id"] = genome_id
        arts_bgctable.index = [f"{i}__arts_bgc" for i in arts_bgctable["bgc_id"]]
    logging.info(f"Writing BGC Table JSON to: {outfile_summary}")
    arts_bgctable_dict = arts_bgctable.T.to_dict()
    with open(outfile_summary, "w") as f:
        json.dump(arts_bgctable_dict, f, indent=2)

    logging.info("Extracting hits...")
    arts_bgctable_hits = {}
    for i in arts_bgctable.index:
        genelist = ast.literal_eval(arts_bgctable.loc[i, "Genelist"])
        antismash_region = arts_bgctable.loc[i, "bgc_id"]
        scaffold = arts_bgctable.loc[i, "Source"].split("_")[-1]
        for num, gene in enumerate(genelist):
            core_gene = gene[1]
            sequence_id = gene[0]
            result = {
                "sequence_id": sequence_id,
                "bgc_id": antismash_region,
                "genome_id": genome_id,
                "gene": core_gene,
                "start": gene[2],
                "stop": gene[3],
                "type": gene[4],
                "description": gene[5],
                "function": gene[6],
                "bgctable_fkey": i,
                "bgc_proximity": "Yes",
            }
            arts_bgctable_hits[
                f"{genome_id}__{core_gene}__{scaffold}__{sequence_id}"
            ] = result

    if outfile_hits is not None:
        outfile_hits.parent.mkdir(parents=True, exist_ok=True)
        logging.info(f"Writing BGC Hits JSON to: {outfile_hits}")
        with open(outfile_hits, "w") as f:
            json.dump(arts_bgctable_hits, f, indent=2)

    return pd.DataFrame.from_dict(arts_bgctable_hits).T


def extract_arts_duptable(
    arts_duptable_tsv, outfile_summary, outfile_hits=None, genome_id=None
):
    """
    Extract information from ARTS Duplication table and write to JSON files.

    This function reads the ARTS Duplication table from a TSV file, generates primary keys
    for the duplication table, extracts hits, and writes the duplication table and hits
    information to JSON files. It returns a DataFrame representation of the hits information.

    Parameters
    ----------
    arts_duptable_tsv : str or pathlib.Path
        Path to the ARTS Duplication table TSV file.

    outfile_summary : str or pathlib.Path
        Path to the output JSON file where the Duplication table information will be written.

    outfile_hits : str or pathlib.Path, optional
        Path to the output JSON file where the hits information will be written.
        If not provided, the hits information will not be written to a file.

    genome_id : str, optional
        Identifier for the genome. If not provided, it will be inferred from the
        Duplication table information.

    Returns
    -------
    pandas.DataFrame
        A DataFrame representation of the extracted hits information, with each row
        representing a hit and columns representing hit attributes.

    Example
    -------
    >>> extract_arts_duptable('path/to/duptable.tsv', 'path/to/summary.json', 'path/to/hits.json', genome_id='genome_id')
    >>> # This will read the Duplication table from 'path/to/duptable.tsv', write summary
    >>> # information to 'path/to/summary.json', write hits information to 'path/to/hits.json',
    >>> # and return a DataFrame representing the hits information.
    """
    logging.info(
        f"Extracting ARTS Duplication table information from: {arts_duptable_tsv}"
    )
    arts_duptable = pd.read_csv(arts_duptable_tsv, sep="\t").fillna("")
    arts_duptable.index = [
        f"{genome_id}__arts_dup__{i}" for i in arts_duptable["#Core_gene"]
    ]
    logging.info(f"Writing Duplication Table JSON to: {outfile_summary}")
    arts_duptable["genome_id"] = genome_id
    arts_duptable_dict = arts_duptable.T.to_dict()
    with open(outfile_summary, "w") as f:
        json.dump(arts_duptable_dict, f, indent=2)

    logging.info("Extracting hits...")
    arts_duptable_hits = {}

    for i in arts_duptable.index:
        hits = arts_duptable.loc[i, "[Hits_listed]"].split(";")
        core_gene = arts_duptable.loc[i, "#Core_gene"]
        for hit in hits:
            data = hit.strip("[").strip("]").split("|")
            scaffold, sequence_id = data[4].split("#")[-1].split("=")[-1].split("_")
            result = {
                "core_gene": core_gene,
                "genome_id": data[0].split()[-1],
                "name": data[1],
                "product": data[2],
                "start": data[4].split("#")[0].split()[0],
                "stop": data[4].split("#")[0].split()[1],
                "strand": data[4].split("#")[0].split()[2],
                "scaffold": f"scaffold_{scaffold}",
                "sequence_id": sequence_id,
                "type": data[5].split("=")[-1],
                "transl_table": data[6].split("=")[-1],
                "duptable_fkey": i,
                "duplication": "Yes",
            }
            arts_duptable_hits[
                f"{genome_id}__{core_gene}__{scaffold}__{sequence_id}"
            ] = result

    if outfile_hits is not None:
        outfile_hits.parent.mkdir(parents=True, exist_ok=True)
        logging.info(f"Writing Duplication Hits JSON to: {outfile_hits}")
        with open(outfile_hits, "w") as f:
            json.dump(arts_duptable_hits, f, indent=2)
    return pd.DataFrame.from_dict(arts_duptable_hits).T


def extract_arts_knownhits(arts_knownhits_tsv, outfile_hits, genome_id=None):
    """
    Extract information from ARTS Known Resistance Hits table and write to a JSON file.

    This function reads the ARTS Known Resistance Hits table from a TSV file, extracts
    hits information, and writes the hits information to a JSON file. It returns a DataFrame
    representation of the Known Resistance Hits information.

    Parameters
    ----------
    arts_knownhits_tsv : str or pathlib.Path
        Path to the ARTS Known Resistance Hits table TSV file.

    outfile_hits : str or pathlib.Path
        Path to the output JSON file where the hits information will be written.

    genome_id : str, optional
        Identifier for the genome. If not provided, it will be inferred from the
        Known Resistance Hits table information.

    Returns
    -------
    pandas.DataFrame
        A DataFrame representation of the extracted Known Resistance Hits information,
        with each row representing a hit and columns representing hit attributes.
        If the input DataFrame is empty, an empty DataFrame is returned.

    Example
    -------
    >>> extract_arts_knownhits('path/to/knownhits.tsv', 'path/to/hits.json', genome_id='genome_id')
    >>> # This will read the Known Resistance Hits table from 'path/to/knownhits.tsv',
    >>> # write hits information to 'path/to/hits.json', and return a DataFrame representing
    >>> # the Known Resistance Hits information.
    """
    logging.info(
        f"Extracting ARTS Known Resistance Hits table information from: {arts_knownhits_tsv}"
    )
    arts_knownhits = pd.read_csv(arts_knownhits_tsv, sep="\t").fillna("")
    for i in arts_knownhits.index:
        hits = arts_knownhits.loc[i, "Sequence description"]
        data = hits.split("|")
        model = arts_knownhits.loc[i, "#Model"]
        sequence_id = arts_knownhits.loc[i, "Sequence id"]
        scaffold = data[2].split("_")[-1]
        result = {
            "gid": data[0],
            "seqtitle": data[1],
            "scaffold": scaffold,
            "start": data[-1].split("_")[0],
            "stop": data[-1].split("_")[1],
            "strand": data[-1].split("_")[2],
            "genome_id": genome_id,
            "known_target": "Yes",
        }
        for k, v in result.items():
            arts_knownhits.loc[i, str(k)] = str(v)
        arts_knownhits.loc[
            i, "index"
        ] = f"{str(genome_id)}__{str(model)}__{str(scaffold)}__{str(sequence_id)}"
    arts_knownhits = (
        arts_knownhits.drop(columns=["Sequence description"])
        .rename(
            columns={
                "#Model": "model",
                "Description": "description",
                "Sequence id": "sequence_id",
            }
        )
    )
    if arts_knownhits.empty:
        logging.warning("ARTS Known Resistance Hits table is empty.")
    else:
        arts_knownhits = arts_knownhits.set_index("index")
    arts_knownhits_dict = arts_knownhits.T.to_dict()
    outfile_hits.parent.mkdir(parents=True, exist_ok=True)
    logging.info(f"Writing Known Resistance Hits JSON to: {outfile_hits}")
    with open(outfile_hits, "w") as f:
        json.dump(arts_knownhits_dict, f, indent=2)
    return arts_knownhits


def extract_arts(arts_dir, genome_id, input_as_json, outdir):
    """
    Extract and summarize information from various ARTS tables and write to JSON files.

    This function reads various ARTS tables from a specified directory, extracts and summarizes
    the information from these tables, and writes the summarized information to JSON files. It
    acts as a wrapper function that calls other functions to handle specific tables and takes
    care of organizing the output and writing it to the files.

    Parameters
    ----------
    arts_dir : str or pathlib.Path
        Path to the directory containing ARTS tables.

    genome_id : str
        Identifier for the genome.

    input_as_json : str or pathlib.Path
        Path to the input JSON file containing antiSMASH results.

    outdir : str or pathlib.Path
        Path to the output directory where the JSON files will be written.

    Returns
    -------
    None
        This function doesn’t return any value. It writes the output to the specified files.

    Outputs
    -------
    ARTS Core Table Summary JSON:
        Contains the summarized information extracted from the ARTS core table.

    ARTS Core Table Hits JSON:
        Contains detailed hits information extracted from the ARTS core table.

    ARTS BGC Table Summary JSON:
        Contains the summarized information extracted from the ARTS BGC table.

    ARTS BGC Table Hits JSON:
        Contains detailed hits information extracted from the ARTS BGC table.

    ARTS Duplication Table Summary JSON:
        Contains the summarized information extracted from the ARTS duplication table.

    ARTS Duplication Table Hits JSON:
        Contains detailed hits information extracted from the ARTS duplication table.

    ARTS Known Resistance Hits JSON:
        Contains detailed hits information extracted from the ARTS known resistance hits table.

    ARTS All Hits JSON:
        Contains a summarized view of all hits extracted from the different ARTS tables.

    Example
    -------
    >>> extract_arts('path/to/arts_dir', 'genome_id', 'path/to/input_as.json', 'path/to/outdir')
    >>> # This will read the ARTS tables from 'path/to/arts_dir', extract and summarize the
    >>> # information, and write the output JSON files to 'path/to/outdir'.
    """
    logging.info(f"Reading ARTS directory from: {arts_dir}")
    arts_tsv = {i.stem: i for i in Path(arts_dir).glob("tables/*.tsv")}

    logging.debug(f"Found ARTS tables: {', '.join(arts_tsv.keys())}\n")
    df_arts = pd.read_csv(arts_tsv["bgctable"], sep="\t")
    with open(input_as_json, "r") as f:
        as_json = json.load(f)

    logging.info("Generating mapping files...")
    hit_mapper, arts_dict, contig_mapper = generate_arts_mapping(df_arts, as_json)
    logging.debug("Done!\n")

    outdir = Path(outdir)
    outdir.mkdir(parents=True, exist_ok=True)

    logging.info("Extracting core table information...")
    arts_coretable_tsv = arts_tsv["coretable"]
    outfile_summary = outdir / f"{genome_id}_arts_coretable_summary.json"
    outfile_hits = outdir / f"{genome_id}_arts_coretable_hits.json"
    coretable_hits = extract_arts_coretable(
        arts_coretable_tsv, outfile_summary, genome_id=genome_id
    )
    logging.debug("Done!\n")

    logging.info("Extracting bgc table information...")
    arts_bgctable_tsv = arts_tsv["bgctable"]
    outfile_summary = outdir / f"{genome_id}_arts_bgctable_summary.json"
    outfile_hits = outdir / f"{genome_id}_arts_bgctable_hits.json"
    bgctable_hits = extract_arts_bgctable(
        arts_bgctable_tsv, outfile_summary, hit_mapper=hit_mapper, genome_id=genome_id
    )
    logging.debug("Done!\n")

    logging.info("Extracting duplication table information...")
    arts_duptable_tsv = arts_tsv["duptable"]
    outfile_summary = outdir / f"{genome_id}_arts_duptable_summary.json"
    outfile_hits = outdir / f"{genome_id}_arts_duptable_hits.json"
    duptable_hits = extract_arts_duptable(
        arts_duptable_tsv, outfile_summary, genome_id=genome_id
    )
    logging.debug("Done!\n")

    logging.info("Extracting known hits table information...")
    arts_knownhits_tsv = arts_tsv["knownhits"]
    outfile_hits = outdir / f"{genome_id}_arts_knownhits.json"
    knownhits = extract_arts_knownhits(
        arts_knownhits_tsv, outfile_hits, genome_id=genome_id
    )
    logging.debug("Done!\n")

    logging.info("Summarizing all hits...")
    df = pd.concat(
        [
            coretable_hits.rename(columns={"core_gene": "core_gene_or_model"}),
            bgctable_hits.rename(columns={"core_gene": "core_gene_or_model"}),
            duptable_hits.rename(columns={"core_gene": "core_gene_or_model"}),
            knownhits.rename(columns={"model": "core_gene_or_model"}),
        ]
    ).sort_index()
    agg_dict = {
        col: lambda series: series.dropna().iloc[0]
        if len(series.dropna()) > 0
        else np.nan
        for col in df.columns
    }
    merged_df = df.groupby(df.index).agg(agg_dict)
    for c in ["duplication", "phylogeny", "known_target", "bgc_proximity"]:
        if c in merged_df.columns:
            merged_df[c] = merged_df[c].fillna("No")
            merged_df[c].replace({"No": False, "Yes": True}, inplace=True)
    outfile = outdir / f"{genome_id}_arts_all_hits.json"
    logging.debug(f"Writing output to {outfile}")
    merged_df.T.to_json(outfile)


if __name__ == "__main__":
    extract_arts(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4])
