def extract_bgc_project_information(config, project_variable="projects", sample_table_index="genome_id"):
    """
    Wrapper to extract variables from projects in config.yaml.
    Returns all necessary objects required to run bgcflow.

    Arguments:
        config:

    Returns:
        df_projects:
        df_samples:
        prokka_db_table:
        prokka_db_map:
        peppy_objects:
    """

    # load information from config
    print(f"Step 1. Extracting project information from config...\n", file=sys.stderr)

    project_variable = 'bgc_projects'
    # Check if the key exists in the dictionary
    if project_variable in config:
        projects = config[project_variable]
    else:
        # Handle the error appropriately
        raise Exception("No 'bgc_project' available. Please add the 'bgc_project' variable to the global configuration.\nThis variable should contain a list of pep projects with a list of antiSMASH region genbanks files as input.\nStopping the workflow.")

    # mask pep and name as alias
    for item in projects:
        if 'pep' in item:
            item['name'] = item.pop('pep')

    # filter for pep projects
    df_projects = pd.DataFrame(projects).set_index("name", drop=False)
    for i in df_projects.index:
        if i.endswith(".yaml"):
            df_projects = df_projects.drop(i)

    # Fill missing df_projects columns
    for item in ["prokka-db", "gtdb-tax", "rules"]:
        if not item in df_projects.columns.tolist():
            df_projects = df_projects.reindex(
                columns=df_projects.columns.tolist() + [item]
            )

    # generate containers to capture output
    df_samples = []
    prokka_db_table = {}
    prokka_db_map = {}
    peppy_objects = {}

    for num, p in enumerate(projects):
        print(
            f"Step 2.{num+1} Getting sample information for project: {p['name']}",
            file=sys.stderr,
        )
        # grab a bgcflow pep project
        if p["name"].endswith(".yaml"):
            pep_file = p["name"]
            print(pep_file)
            print(f" - Processing project {pep_file}", file=sys.stderr)
            p = peppy.Project(pep_file, sample_table_index=sample_table_index)


            # make sure each project has unique names
            assert (
                not p.name in df_projects["name"].unique()
            ), f"Project name [{p.name}] in [{pep_file}] has been used. Please use different name for each project."

            df_projects.loc[p.name, "name"] = p.name
            df_projects.loc[p.name, "samples"] = p.config_file
            df_projects.loc[p.name, "rules"] = p.config_file

        # grab a bgcflow project parameters from main config.yaml
        # exist for back compatibility with bgcflow=<0.3.3
        else:
            p_bgcflow = peppy.Project(p["samples"], sample_table_index=sample_table_index)
            print(f" - Processing project [{p['name']}]", file=sys.stderr)
            p = refine_bgcflow_project(p_bgcflow, p)

        # populate input files for custom samples
        p.sample_tables = get_input_location(p, force_extension="gbk")

        # grab global rule config if rule not presents
        if "rules" not in p.config.keys():
            p.config["rules"] = config["rules"]

        df_sample, df_gtdb, prokka_db_table, prokka_db_map = read_pep_project(
            p, prokka_db_table, prokka_db_map
        )
        peppy_objects[p.name] = p

        # Only to accommodate peppy<=0.34.0
        for item in ["closest_placement_reference", "genus"]:
            if not item in df_sample.columns.tolist():
                df_sample = df_sample.reindex(
                    columns=df_sample.columns.tolist() + [item]
                )

        df_samples.append(df_sample.replace("NaN", ""))
    df_samples = pd.concat(df_samples)

    # Fill missing df_samples columns
    for item in [
        "organism",
        "genus",
        "species",
        "strain",
        "closest_placement_reference",
        "input_file",
    ]:
        if not item in df_samples.columns.tolist():
            df_samples = df_samples.reindex(
                columns=df_samples.columns.tolist() + [item]
            )

    #print(f"Step 3 Merging genome_ids across projects...\n", file=sys.stderr)
    #df_samples = df_samples.fillna("")
    #try:
    #    if 'BGC' in [peppy_objects[k].config['schema'] for k in peppy_objects.keys()]:
    #        pass
    #    else:
    #        df_samples = find_conflicting_samples(df_samples)
    #except KeyError:
    #    df_samples = find_conflicting_samples(df_samples)
    # print(f"Step 4 Checking validity of samples using schemas..\n", file=sys.stderr)
    # validate(df_samples, schema="schemas/samples.schema.yaml")

    return df_projects, df_samples, prokka_db_table, prokka_db_map, peppy_objects

report: "report/workflow.rst"


include: "rules/common.smk"


##### 1. Extract information from config file
(
    DF_PROJECTS,
    DF_SAMPLES,
    PROKKA_DB_TABLE,
    PROKKA_DB_MAP,
    PEP_PROJECTS,
) = extract_bgc_project_information(config, project_variable="bgc_projects", sample_table_index="bgc_id") # specify project type

# generate centralized sample datasets
bgcflow_util_dir = Path("data/interim/bgcflow_utils")
bgcflow_util_dir.mkdir(parents=True, exist_ok=True)
DF_SAMPLES.to_csv(bgcflow_util_dir / "samples.csv", index=False)

##### 2. Generate wildcard constants #####
PROJECT_IDS = list(DF_PROJECTS.name.unique())
STRAINS = DF_SAMPLES.genome_id.to_list()
BGCS = DF_SAMPLES.bgc_id.to_list()
CUSTOM = DF_SAMPLES[DF_SAMPLES.source.eq("custom")].genome_id.to_list()
NCBI = DF_SAMPLES[DF_SAMPLES.source.eq("ncbi")].genome_id.to_list()
PATRIC = DF_SAMPLES[DF_SAMPLES.source.eq("patric")].genome_id.to_list()
SAMPLE_PATHS = list(DF_PROJECTS.samples.unique())
GTDB_PATHS = [
    str(PEP_PROJECTS[k].config["gtdb-tax"])
    for k in PEP_PROJECTS.keys()
    if "gtdb-tax" in PEP_PROJECTS[k].config.keys()
]
PROKKA_GBFF = list(PROKKA_DB_TABLE.keys())

##### 3. Wildcard constraints #####
wildcard_constraints:
    strains="|".join(STRAINS),
    ncbi="|".join(NCBI),
    custom="|".join(CUSTOM),
    patric="|".join(PATRIC),
    name="|".join(PROJECT_IDS),
    prokka_db="|".join(PROKKA_GBFF),
    bgc="|".join(BGCS),

##### 4. Generate user-defined local resources
custom_resource_dir()

##### Target rules #####
rule all:
    input:
        expand("data/processed/{name}/tables/df_gtdb_meta.csv", name=PROJECT_IDS),
        get_final_output(DF_SAMPLES, PEP_PROJECTS, rule_dict_path="workflow/rules_bgc.yaml"),

include: "rules/gtdb.smk"
include: "rules/bgc_selection.smk" # need to be better
include: "rules/bigscape.smk"
include: "rules/bgc_analytics.smk"
include: "rules/antismash.smk"
include: "rules/bigslice.smk"
include: "rules/clinker.smk"
include: "rules/interproscan.smk"
include: "rules/mmseqs2.smk"
include: "rules/getphylo.smk"
